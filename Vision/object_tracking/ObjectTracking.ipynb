{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9c41708-41d2-4480-a5f4-80ad1fdcda0e",
   "metadata": {},
   "source": [
    "## Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96149cd-df79-4751-aa33-cdbdcd62d6b3",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc9c8a-2752-4e0f-a24a-47efdaabc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "roi_defined = False\n",
    " \n",
    "def define_ROI(event, x, y, flags, param):\n",
    "\tglobal r,c,w,h,roi_defined\n",
    "\t# if the left mouse button was clicked, \n",
    "\t# record the starting ROI coordinates \n",
    "\tif event == cv2.EVENT_LBUTTONDOWN:\n",
    "\t\tr, c = x, y\n",
    "\t\troi_defined = False\n",
    "\t# if the left mouse button was released,\n",
    "\t# record the ROI coordinates and dimensions\n",
    "\telif event == cv2.EVENT_LBUTTONUP:\n",
    "\t\tr2, c2 = x, y\n",
    "\t\th = abs(r2-r)\n",
    "\t\tw = abs(c2-c)\n",
    "\t\tr = min(r,r2)\n",
    "\t\tc = min(c,c2)  \n",
    "\t\troi_defined = True\n",
    "\n",
    "cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Ball.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Basket.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Car.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Sunshade.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Woman.mp4')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    " \n",
    "while True:\n",
    "\t# display the image and wait for a keypress\n",
    "\tcv2.imshow(\"First image\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t# if the ROI is defined, draw it!\n",
    "\tif (roi_defined):\n",
    "\t\t# draw a green rectangle around the region of interest\n",
    "\t\tcv2.rectangle(frame, (r,c), (r+h,c+w), (0, 255, 0), 2)\n",
    "\t# else reset the image...\n",
    "\telse:\n",
    "\t\tframe = clone.copy()\n",
    "\t# if the 'q' key is pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    " \n",
    "track_window = (r,c,h,w)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[c:c+w, r:r+h]\n",
    "# conversion to Hue-Saturation-Value space\n",
    "# 0 < H < 180 ; 0 < S < 255 ; 0 < V < 255\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "# computation mask of the histogram:\n",
    "# Pixels with S<30, V<20 or V>235 are ignored \n",
    "mask = cv2.inRange(hsv_roi, np.array((0.,30.,20.)), np.array((180.,255.,235.)))\n",
    "\n",
    "\n",
    "# Marginal histogram of the Hue component\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "# Marginal histogram of the S component\n",
    "#roi_hist = cv2.calcHist([hsv_roi],[1],mask,[180],[0,180])\n",
    "\n",
    "\n",
    "# Histogram values are normalised to [0,255]\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria: either 10 iterations,\n",
    "# or move by less than 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "cpt = 1\n",
    "\n",
    "cv2.namedWindow('Back Projection')\n",
    "cv2.namedWindow('Sequence')\n",
    "cv2.namedWindow('Hue')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        hsv_disp = hsv\n",
    "        hsv_disp[:,:,1] = 255\n",
    "        hsv_disp[:,:,2] = 255\n",
    "        huedisp = cv2.cvtColor(hsv_disp, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "        r,c,h,w = track_window\n",
    "        frame_tracked = cv2.rectangle(frame, (r,c), (r+h,c+w), (255,0,0) ,2)\n",
    "        cv2.imshow('Sequence',frame_tracked)\n",
    "        cv2.imshow('Hue', huedisp)\n",
    "\n",
    "        # Show the back projection result in a separate window\n",
    "        cv2.imshow('Back Projection', dst)\n",
    "\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif k == ord('s'):\n",
    "            cv2.imwrite('Frame_%04d.png'%cpt,frame_tracked)\n",
    "        cpt += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e3acf-0514-4e1a-b800-ed63ed5c7a7c",
   "metadata": {},
   "source": [
    "## Mean shift with threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b45443-2fb4-47de-99df-12eb9944a109",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3e31af-fe12-4701-955c-d6a9485e2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "roi_defined = False\n",
    "\n",
    "def define_ROI(event, x, y, flags, param):\n",
    "    global r, c, w, h, roi_defined\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        r, c = x, y\n",
    "        roi_defined = False\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        r2, c2 = x, y\n",
    "        h = abs(r2 - r)\n",
    "        w = abs(c2 - c)\n",
    "        r = min(r, r2)\n",
    "        c = min(c, c2)\n",
    "        roi_defined = True\n",
    "\n",
    "cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Ball.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Basket.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Car.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Sunshade.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Woman.mp4')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"First image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if roi_defined:\n",
    "        cv2.rectangle(frame, (r, c), (r + h, c + w), (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = clone.copy()\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "track_window = (r, c, h, w)\n",
    "roi = frame[c:c + w, r:r + h]\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "mask = cv2.inRange(hsv_roi, np.array((0., 30., 20.)), np.array((180., 255., 235.)))\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        dst_thresholded = dst.copy()\n",
    "        dst_thresholded[dst < 240] = 0\n",
    "        ret, track_window = cv2.meanShift(dst_thresholded, track_window, term_crit)\n",
    "        r, c, h, w = track_window\n",
    "        frame_tracked = cv2.rectangle(frame, (r, c), (r + h, c + w), (255, 0, 0), 2)\n",
    "        cv2.imshow('Sequence', frame_tracked)\n",
    "        cv2.imshow('Thresholded Back Projection', dst_thresholded)\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab50b93-9ab4-45ea-b8ca-f85f55af5339",
   "metadata": {},
   "source": [
    "## Hough Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ce83a-f6b8-4939-ae15-a10209cf6d9e",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a11bdb9-200a-4799-b9ef-03cf35c7994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "roi_defined = False\n",
    " \n",
    "def define_ROI(event, x, y, flags, param):\n",
    "    global r, c, w, h, roi_defined\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        r, c = x, y\n",
    "        roi_defined = False\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        r2, c2 = x, y\n",
    "        h = abs(r2 - r)\n",
    "        w = abs(c2 - c)\n",
    "        r = min(r, r2)\n",
    "        c = min(c, c2)  \n",
    "        roi_defined = True\n",
    "\n",
    "#cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Ball.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Basket.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Car.mp4')\n",
    "cap = cv2.VideoCapture('./Test-Videos/VOT-Sunshade.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Woman.mp4')\n",
    "\n",
    "\n",
    "ret, frame = cap.read()\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"First image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if roi_defined:\n",
    "        cv2.rectangle(frame, (r, c), (r + h, c + w), (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = clone.copy()\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "track_window = (r, c, h, w)\n",
    "roi = frame[c:c+w, r:r+h]\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "mask = cv2.inRange(hsv_roi, np.array((0., 30., 20.)), np.array((180., 255., 235.)))\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "cpt = 1\n",
    "\n",
    "cv2.namedWindow('Original')\n",
    "cv2.namedWindow('Gradient Orientation')\n",
    "cv2.namedWindow('Gradient Magnitude')\n",
    "cv2.namedWindow('Selected Orientations')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        hsv_disp = hsv.copy()\n",
    "        hsv_disp[:,:,1] = 255\n",
    "        hsv_disp[:,:,2] = 255\n",
    "        huedisp = cv2.cvtColor(hsv_disp, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Calculate gradient orientation and magnitude\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        grad_mag, grad_ori = cv2.cartToPolar(grad_x, grad_y, angleInDegrees=True)\n",
    "\n",
    "        # Apply threshold on gradient magnitude\n",
    "        mask = grad_mag > 50  # Adjust threshold as needed\n",
    "\n",
    "        # Create a copy of frame for visualization\n",
    "        frame_with_orientation = frame.copy()\n",
    "\n",
    "        # Mask pixels with insignificant gradient magnitudes\n",
    "        frame_with_orientation[mask == 0] = [0, 0, 255]  # Set insignificant gradient pixels to red\n",
    "\n",
    "        # Display images\n",
    "        cv2.imshow('Original', frame)\n",
    "        cv2.imshow('Gradient Orientation', grad_ori.astype(np.uint8))\n",
    "        cv2.imshow('Gradient Magnitude', grad_mag.astype(np.uint8))\n",
    "        cv2.imshow('Selected Orientations', frame_with_orientation)\n",
    "\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif k == ord('s'):\n",
    "            cv2.imwrite('Frame_%04d.png' % cpt, frame)\n",
    "        cpt += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9991bc1f-7571-4877-9820-814eb9c7a4b7",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "## Tracking using Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd5934d-4fc8-442c-88b9-da68e1b3aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "roi_defined = False\n",
    "\n",
    "#cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Ball.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Basket.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Car.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Sunshade.mp4')\n",
    "cap = cv2.VideoCapture('./Test-Videos/VOT-Woman.mp4')\n",
    "\n",
    "def define_ROI(event, x, y, flags, param):\n",
    "    global r, c, w, h, roi_defined\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        r, c = x, y\n",
    "        roi_defined = False\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        r2, c2 = x, y\n",
    "        h = abs(r2 - r)\n",
    "        w = abs(c2 - c)\n",
    "        r = min(r, r2)\n",
    "        c = min(c, c2)\n",
    "        roi_defined = True\n",
    "\n",
    "def calculate_gradient_orientation(frame, threshold):\n",
    "    blue_channel = frame[..., 2]\n",
    "    gradient_x, gradient_y = np.gradient(blue_channel)\n",
    "    gradient_magnitude = np.sqrt(gradient_x ** 2 + gradient_y ** 2)\n",
    "    orientation = np.arctan2(gradient_y, gradient_x)\n",
    "    not_valid_indices = gradient_magnitude < threshold\n",
    "    valid_indices = gradient_magnitude > threshold\n",
    "    return gradient_magnitude, orientation, np.where(not_valid_indices), np.where(valid_indices)\n",
    "\n",
    "def accumulate_hough_space(t_hough, orientation_map, r_table, valid_indices):\n",
    "    for px, py in zip(valid_indices[0], valid_indices[1]):\n",
    "        angle_index = int(orientation_map[px, py] * 90 / math.pi)\n",
    "        if angle_index in r_table:\n",
    "            for value in r_table[angle_index]:\n",
    "                new_py, new_px = py + value[0], px + value[1]\n",
    "                if 0 <= new_py < t_hough.shape[1] and 0 <= new_px < t_hough.shape[0]:\n",
    "                    t_hough[new_px, new_py] += 1\n",
    "    return t_hough\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if ret is False:\n",
    "    print(\"Unable to read the first frame from the video.\")\n",
    "    exit(1)\n",
    "\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"First image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if roi_defined:\n",
    "        cv2.rectangle(frame, (r, c), (r + h, c + w), (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = clone.copy()\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "track_window = (r, c, h, w)\n",
    "roi = frame[c:c + w, r:r + h]\n",
    "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "mask = cv2.inRange(hsv_roi, np.array((0., 30., 20.)), np.array((180., 255., 235.)))\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "R = defaultdict(list)\n",
    "thresh = 25\n",
    "grad, orient, _, ind = calculate_gradient_orientation(hsv_roi, thresh)\n",
    "centroid = np.array([int(r + (h // 2)), int(c + (w // 2))])\n",
    "orient = (orient * 90 / np.pi).astype(int)\n",
    "\n",
    "for x, y in zip(ind[0], ind[1]):\n",
    "    distance = centroid - np.array([y + r, x + c])\n",
    "    R[orient[x, y]].append(distance)\n",
    "\n",
    "cpt = 1\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret is False:\n",
    "        print(\"End of video reached.\")\n",
    "        break\n",
    "\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    grad, orient, _, ind = calculate_gradient_orientation(frame_hsv, thresh)\n",
    "    hough_transform = np.zeros_like(orient)\n",
    "    hough_transform = accumulate_hough_space(hough_transform, orient, R, ind)\n",
    "\n",
    "    # Find the peak in the Hough space\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(hough_transform)\n",
    "    r, c = max_loc\n",
    "\n",
    "    frame_tracker = cv2.rectangle(frame, (c, r), (c + w, r + h), (255, 0, 0), 2)\n",
    "    hough_transform = np.uint8(hough_transform)\n",
    "    hough_transform = cv2.normalize(hough_transform, hough_transform, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    cv2.imshow('Sequences', frame_tracker)\n",
    "    cv2.imshow('Hough Transform', hough_transform)\n",
    "\n",
    "    k = cv2.waitKey(60) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv2.imwrite('./images/Frame_%04d.png' % cpt, frame_tracker)\n",
    "        cv2.imwrite('./images/Hough_%04d.png' % cpt, hough_transform)\n",
    "    cpt += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1a8f2-babf-4e71-8396-f6cb2e8759a1",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "## Tracking using both Hough Transform and Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac87f0d5-c373-4709-a041-293d65b3e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "roi_defined = False\n",
    "\n",
    "cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Ball.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Basket.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Car.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Sunshade.mp4')\n",
    "#cap = cv2.VideoCapture('./Test-Videos/VOT-Woman.mp4')\n",
    "\n",
    "\n",
    "def define_ROI(event, x, y, flags, param):\n",
    "\tglobal r,c,w,h,roi_defined\n",
    "\t# if the left mouse button was clicked,\n",
    "\t# record the starting ROI coordinates\n",
    "\tif event == cv2.EVENT_LBUTTONDOWN:\n",
    "\t\tr, c = x, y\n",
    "\t\troi_defined = False\n",
    "\t# if the left mouse button was released,\n",
    "\t# record the ROI coordinates and dimensions\n",
    "\telif event == cv2.EVENT_LBUTTONUP:\n",
    "\t\tr2, c2 = x, y\n",
    "\t\th = abs(r2-r)\n",
    "\t\tw = abs(c2-c)\n",
    "\t\tr = min(r,r2)\n",
    "\t\tc = min(c,c2)\n",
    "\t\troi_defined = True\n",
    "        \n",
    "#implementing gradient orientation without using opencv\n",
    "def calculate_gradient_orientation(frame, threshold):\n",
    "    \"\"\"\n",
    "    Calculates gradient orientation based on the blue channel of the input frame.\n",
    "\n",
    "    Args:\n",
    "    - frame (numpy.ndarray): Input frame, assumed to be in BGR format.\n",
    "    - threshold (float): Magnitude threshold for valid gradients.\n",
    "\n",
    "    Returns:\n",
    "    - gradient_magnitude (numpy.ndarray): Magnitude of gradients.\n",
    "    - orientation (numpy.ndarray): Orientation of gradients in radians.\n",
    "    - orientation_bgr (numpy.ndarray): Visualization of valid orientations in BGR format.\n",
    "    - not_valid_indices (tuple of numpy.ndarray): Indices where gradient magnitude is below threshold.\n",
    "    - valid_indices (tuple of numpy.ndarray): Indices where gradient magnitude is above threshold.\n",
    "    \"\"\"\n",
    "    # Extract the blue channel directly for gradient calculation\n",
    "    blue_channel = frame[..., 2]\n",
    "\n",
    "    # Calculating the gradient\n",
    "    gradient_x, gradient_y = np.gradient(blue_channel)\n",
    "    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "\n",
    "    # Calculating the orientation\n",
    "    orientation = np.arctan2(gradient_y, gradient_x)\n",
    "\n",
    "    # Finding indices for valid and not valid gradients\n",
    "    not_valid_indices = gradient_magnitude < threshold\n",
    "    valid_indices = gradient_magnitude > threshold\n",
    "\n",
    "    # Creating a visualization of valid orientations\n",
    "    orientation_normalized = cv2.normalize(orientation, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    orientation_bgr = cv2.cvtColor(np.uint8(orientation_normalized), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Marking points not used as red\n",
    "    orientation_bgr[not_valid_indices] = [0, 0, 255]\n",
    "\n",
    "    return gradient_magnitude, orientation, orientation_bgr, np.where(not_valid_indices), np.where(valid_indices)\n",
    "\n",
    "\n",
    "\n",
    "def accumulate_hough_space(t_hough, orientation_map, r_table, valid_indices):\n",
    "    \"\"\"\n",
    "    Accumulates Hough space based on orientation and r-table.\n",
    "\n",
    "    Args:\n",
    "    - t_hough (numpy.ndarray): The Hough space accumulator array.\n",
    "    - orientation_map (numpy.ndarray): Map of orientations.\n",
    "    - r_table (dict): Dictionary containing R-table values.\n",
    "    - valid_indices (tuple): Tuple containing arrays of valid indices.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Accumulated Hough space.\n",
    "    \"\"\"\n",
    "    for px, py in zip(valid_indices[0], valid_indices[1]):\n",
    "        angle_index = int(orientation_map[px, py] * 90 / math.pi)\n",
    "        if angle_index in r_table:\n",
    "            for value in r_table[angle_index]:\n",
    "                new_py, new_px = py + value[0], px + value[1]\n",
    "                if 0 <= new_py < t_hough.shape[1] and 0 <= new_px < t_hough.shape[0]:\n",
    "                    t_hough[new_px, new_py] += 1\n",
    "\n",
    "    return t_hough\n",
    "\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "# load the image, clone it, and setup the mouse callback function\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    "\n",
    "# keep looping until the 'q' key is pressed\n",
    "while True:\n",
    "\t# display the image and wait for a keypress\n",
    "\tcv2.imshow(\"First image\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t# if the ROI is defined, draw it!\n",
    "\tif (roi_defined):\n",
    "\t\t# draw a green rectangle around the region of interest\n",
    "\t\tcv2.rectangle(frame, (r,c), (r+h,c+w), (0, 255, 0), 2)\n",
    "\t# else reset the image...\n",
    "\telse:\n",
    "\t\tframe = clone.copy()\n",
    "\t# if the 'q' key is pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "track_window = (r,c,h,w)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[c:c+w, r:r+h]\n",
    "# conversion to Hue-Saturation-Value space\n",
    "# 0 < H < 180 ; 0 < S < 255 ; 0 < V < 255\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "# computation mask of the histogram:\n",
    "# Pixels with S<30, V<20 or V>235 are ignored\n",
    "mask = cv2.inRange(hsv_roi, np.array((0.,30.,20.)), np.array((180.,255.,235.)))\n",
    "# Marginal histogram of the Hue component\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "# Histogram values are normalised to [0,255]\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria: either 10 iterations,\n",
    "# or move by less than 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "\n",
    "\n",
    "R = defaultdict(list)\n",
    "thresh = 25\n",
    "# Populating the table\n",
    "grad, orient, _, _, ind = calculate_gradient_orientation(hsv_roi, thresh)\n",
    "centroid = np.array([int(r + (h//2)), int(c + (w//2))])\n",
    "orient = (orient * 90 / np.pi).astype(int)\n",
    "\n",
    "\n",
    "for x, y in zip(ind[0], ind[1]):\n",
    "    distance = centroid - np.array([y + r, x + c])\n",
    "    R[orient[x, y]].append(distance)\n",
    "\n",
    "cpt = 1\n",
    "while(1):\n",
    "\tret ,frame = cap.read()\n",
    "\tif ret == True:\n",
    "\n",
    "\t\tfram_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\t\tgrad, orient , ori, _, ind = calculate_gradient_orientation(fram_hsv, thresh)\n",
    "\t\though_transform = np.zeros_like(orient)\n",
    "\n",
    "\t\though_transform = accumulate_hough_space(hough_transform, orient, R, ind)\n",
    "\n",
    "\t\t#mean shift\n",
    "\t\tret, track_window = cv2.meanShift(hough_transform, track_window, term_crit)\n",
    "\t\tr,c,h,w = track_window\n",
    "\n",
    "\t\t# Draw a blue rectangle on the current image and normalize Hough\n",
    "\t\tframe_tracker = cv2.rectangle(frame, (r, c), (r + h, c + w), (255, 0, 0), 2)\n",
    "\t\though_transform = np.uint8(hough_transform)\n",
    "\t\though_transform = cv2.normalize(hough_transform, hough_transform, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "\t\t#Plotting all images\n",
    "\t\tcv2.imshow('Sequences', frame_tracker)\n",
    "\t\tcv2.imshow('Orientations', ori)\n",
    "\t\tcv2.imshow(\"Hough Transorm\", hough_transform)\n",
    "\n",
    "\t\tk = cv2.waitKey(60) & 0xff\n",
    "\t\tif k == 27:\n",
    "\t\t\t\tbreak\n",
    "\t\telif k == ord('s'):\n",
    "\t\t\t\tcv2.imwrite('./images/Question5_Frame_%04d.png'%cpt,frame_tracked)\n",
    "\t\t\t\tcv2.imwrite('./images/Question5_tHough_%04d.png'%cpt,tHough)\n",
    "\t\t\t\tcv2.imwrite('./images/Question5_Orientation_%04d.png'%cpt,ori)\n",
    "\t\tcpt += 1\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513919ef-0345-4d15-9967-f0a46b70d488",
   "metadata": {},
   "source": [
    "## Deep Features\n",
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ced1f81-03a6-4b54-9c8b-0cfe7cabb0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mojan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Mojan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Mojan\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "ROI defined: 107 88 68 97\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\mean.dispatch.cpp:615: error: (-215:Assertion failed) dst.type() == CV_64F && dst.isContinuous() && (dst.cols == 1 || dst.rows == 1) && dcn >= cn in function 'cv::meanStdDev'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m target_features \u001b[38;5;241m=\u001b[39m target_features\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Calculate similarity\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m similarity \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmatchTemplate(frame_features[\u001b[38;5;241m0\u001b[39m], target_features[\u001b[38;5;241m0\u001b[39m], cv2\u001b[38;5;241m.\u001b[39mTM_CCOEFF_NORMED)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Find the maximum value of similarity\u001b[39;00m\n\u001b[0;32m     70\u001b[0m min_val, max_val, min_loc, max_loc \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mminMaxLoc(similarity)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\mean.dispatch.cpp:615: error: (-215:Assertion failed) dst.type() == CV_64F && dst.isContinuous() && (dst.cols == 1 || dst.rows == 1) && dcn >= cn in function 'cv::meanStdDev'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "roi_defined = False\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_features(frame):\n",
    "    frame = cv2.resize(frame, (224, 224))\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    frame = frame.astype('float32')\n",
    "    frame = frame / 255.0\n",
    "\n",
    "    features = model.predict(frame)\n",
    "    return features\n",
    "\n",
    "def define_ROI(event, x, y, flags, param):\n",
    "    global r, c, w, h, roi_defined\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        r, c = x, y\n",
    "        roi_defined = False\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        r2, c2 = x, y\n",
    "        h = abs(r2-r)\n",
    "        w = abs(c2-c)\n",
    "        r = min(r,r2)\n",
    "        c = min(c,c2)\n",
    "        roi_defined = True\n",
    "        print(\"ROI defined:\", r, c, h, w)\n",
    "\n",
    "cap = cv2.VideoCapture('./Test-Videos/Antoine_Mug.mp4')\n",
    "ret, frame = cap.read()\n",
    "clone = frame.copy()\n",
    "cv2.namedWindow(\"First image\")\n",
    "cv2.setMouseCallback(\"First image\", define_ROI)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"First image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if roi_defined:\n",
    "        cv2.rectangle(frame, (r, c), (r + h, c + w), (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = clone.copy()\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "track_window = (r, c, h, w)\n",
    "\n",
    "roi = frame[c:c+w, r:r+h]\n",
    "target_features = extract_features(roi)\n",
    "\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "cv2.namedWindow('Sequence')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        frame_features = extract_features(frame)\n",
    "\n",
    "        # Ensure the correct data type and continuity\n",
    "        frame_features = frame_features.astype(np.float32)\n",
    "        target_features = target_features.astype(np.float32)\n",
    "\n",
    "        # Calculate similarity\n",
    "        similarity = cv2.matchTemplate(frame_features[0], target_features[0], cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        # Find the maximum value of similarity\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(similarity)\n",
    "\n",
    "        # Update track window based on maximum similarity location\n",
    "        r, c = max_loc\n",
    "        track_window = (r, c, h, w)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (r, c), (r+h, c+w), (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow('Sequence', frame)\n",
    "\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_KERNEL",
   "language": "python",
   "name": "gpu_prat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
